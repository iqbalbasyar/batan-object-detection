{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4b4bc60-d79e-4676-9c38-598304ff83a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import PIL # install using pip install pillow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66845b61-b4a5-4ff3-8cf6-61885f9c67e4",
   "metadata": {},
   "source": [
    "# Object Detection "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b67a4d-c715-454f-a2ad-5eb8304cdf8f",
   "metadata": {},
   "source": [
    "In previous course, we already have learned on how to make image classificatoin model using Tensorlow and Keras respectively. If we look the object detection task, it was a combination of **what** and **where**. In other words, to achieve object detection we also need to do classification and localization task. This graph below see the common roadmap of computer vision in past development. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da037058-5f48-4030-a557-d22119eb2767",
   "metadata": {},
   "source": [
    "![](https://machinelearningmastery.com/wp-content/uploads/2019/05/Object-Recognition.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b149608d-fcc1-4ef0-9c28-b8c96fb343ad",
   "metadata": {},
   "source": [
    "As its task contain of two subtask, you may guessed that the architecture will have **2 objective** function. The general and early Deep Learning architecture of object detection can be tracked on Region Based CNN (R-CNN), which learn how to create a box and classifiy what's inside it. \n",
    "\n",
    "The R-CNN Architecture consisted of three main components:\n",
    "- Region Selector : Select and Extract region that might represents an object, called Region of Iterest (ROI)\n",
    "- CNN : Process every warped region (why would every ROI need to be warped?)\n",
    "- Classifier : Consisted of shallow dense network to classify the processed ROI "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893eff5b-7ed5-4fd4-8a8c-798f45f8a223",
   "metadata": {},
   "source": [
    "![](https://i0.wp.com/bdtechtalks.com/wp-content/uploads/2021/06/R-CNN-architecture.jpg?w=1392&ssl=1)\n",
    "\n",
    "The architecture then evolving to become Fast R-CNN which faster than R-CNN because it combines the 1st and 2nd process of R-CNN model into one CNN model. Fast R-CNN receives an image and a set of RoIs and returns a list of bounding boxes and classes of the objects detected in the image like illustration below\n",
    "![](https://machinelearningmastery.com/wp-content/uploads/2019/03/Summary-of-the-Fast-R-CNN-Model-Architecture.png)\n",
    "\n",
    "Apart from the R-CNN model family, there are another family that are widely used in current years : \n",
    "- [YOLO model](https://www.section.io/engineering-education/introduction-to-yolo-algorithm-for-object-detection/)\n",
    "- [SSD model](https://developers.arcgis.com/python/guide/how-ssd-works/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfe5c5f-a80d-49b3-9c5f-ee1b362d7b9c",
   "metadata": {},
   "source": [
    "References:\n",
    "- [R-CNN paper](https://arxiv.org/pdf/1311.2524.pdf)\n",
    "- [Fast R-CNN paper](https://arxiv.org/pdf/1504.08083.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39bf72d-58e9-4649-87de-19cc2c1dbe65",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f14ae5-1e34-45bc-9a79-ff44c41263b3",
   "metadata": {},
   "source": [
    "# Making Object Detection Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a34e7a-460b-4316-b415-d171fb3c4761",
   "metadata": {},
   "source": [
    "A difference between object detection task and classification can be seen from its training data. While classification dataset might only consisted of images inside a specific folder for its class, an object detection will also need additional information regarding what object exists on that specific images and which class it fall into. It's not necessary to organize each class to a folder since there might be multiple object or classes inside one image.\n",
    "\n",
    "![](https://raw.githubusercontent.com/tzutalin/labelImg/master/demo/demo3.jpg)\n",
    "\n",
    "## Create Bounding-box and label \n",
    "In this section we will transform our images inside `data/` repository and do hand-labeling to create object detection dataset. The whole process might be written as these simple flow : \n",
    "\n",
    "- **input** : Raw images \n",
    "- **process** : Manually hand-labeling with 3rd party apps\n",
    "- **output**: Xml files contains box coordinates and its label inside each images\n",
    "\n",
    "\n",
    "**Task 1: Install labelimg** \\\n",
    "In order to do the hand-labeling, we need a tool to help us label the image and create bounding box for each images. Basically you can use any tools, but for convenience we will use python app from https://github.com/tzutalin/labelImg. You can go to its github repo and see the installation guide, or you can just install using `pip install labelimg`.\n",
    "\n",
    "Install it using your terminal, then try to run `labelimg`using terminal and you should see the app is running\n",
    "\n",
    "**Task 2: Hand labeling**\n",
    "1. Open the labelimg app then locate the image using \"open dir\" for training or test images\n",
    "2. Make sure to use \"Pascal/VOC\" format. We will be using this format in this capstone\n",
    "3. Create RectBox on every specific object and label it as you \n",
    "4. If you finished to do all the annotation, click save and name it with exact same filename in a specific folder of your own. It should automatically save the file with .xml extension. Below are the final folder structure should looks like.\n",
    "\n",
    "        +image_folder\n",
    "        | +train\n",
    "        | | -img1.jpg\n",
    "        | | -img1.xml\n",
    "        | | -...\n",
    "        | | -imgn.png\n",
    "        | | -imgn.xml\n",
    "        | +test\n",
    "        | | -img1.jpg\n",
    "        | | -img1.xml\n",
    "        | | -...\n",
    "        | | -imgn.png\n",
    "        | | -imgn.xml\n",
    "\n",
    "7. Select next image and repeat process 5-6 untill all images is annotated\n",
    "5. For model configuration, create/edit `labelmap.pbtxt` and write down your class-mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35712c98-7e35-4eca-afc5-9dafc4359fbd",
   "metadata": {},
   "source": [
    "## Convert XML to CSV file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "168c2753-54d9-4850-a2f7-c48ee0bffe40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully converted xml to csv.\n"
     ]
    }
   ],
   "source": [
    "def xml_to_csv(path):\n",
    "    xml_list = []\n",
    "    for xml_file in glob.glob(path + '/*.xml'):\n",
    "        tree = ET.parse(xml_file)\n",
    "        root = tree.getroot()\n",
    "        for member in root.findall('object'):\n",
    "            value = (root.find('filename').text,\n",
    "                     int(root.find('size')[0].text),\n",
    "                     int(root.find('size')[1].text),\n",
    "                     member[0].text,\n",
    "                     int(member[4][0].text),\n",
    "                     int(member[4][1].text),\n",
    "                     int(member[4][2].text),\n",
    "                     int(member[4][3].text)\n",
    "                     )\n",
    "            xml_list.append(value)\n",
    "    column_name = ['filename', 'width', 'height', 'class', 'xmin', 'ymin', 'xmax', 'ymax']\n",
    "    xml_df = pd.DataFrame(xml_list, columns=column_name)\n",
    "    return xml_df\n",
    "\n",
    "\n",
    "for folder in ['train', 'test']:\n",
    "    image_path = os.path.join(os.getcwd(), ('data/' + folder))\n",
    "    xml_df = xml_to_csv(image_path)\n",
    "    xml_df.to_csv(('data/'+folder+'_labels.csv'), index=None)\n",
    "print('Successfully converted xml to csv.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1f62fa-f82b-451c-b618-1863eb15451d",
   "metadata": {},
   "source": [
    "## Create TFRecord"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caaaa646-fa6d-4695-be42-eca79c2c1aa6",
   "metadata": {},
   "source": [
    "We already provide a python file to generate the tfrecord, `generate_tfrecord.py`. Before you run it, please make sure that the `class_text_to_int` function has the correct label as you stated in `labelmap.pbtxt`. \n",
    "\n",
    "**Task : Adjust `class_text_to_int` function inside `generate_tfrecord.py`**. The function looks like this : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a238991-221e-4914-895c-1bb56c88821d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_text_to_int(row_label):\n",
    "    if row_label =='your_label_1':\n",
    "        return 1\n",
    "    elif row_label =='your_label_2':\n",
    "        return 2\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aadef98-62be-4d28-90b5-1dc552665897",
   "metadata": {},
   "source": [
    "Then on your terminal, move into `generate_tfrecord.py` directory and run the following command to generate train.record:\n",
    "```bash\n",
    "python generate_tfrecord.py --csv_input=<path_to_train_labels.csv> --image_dir=<path_to_train_iamges> --output_path=train.record\n",
    "```\n",
    "\n",
    "And this one to generate test.record :\n",
    "```bash\n",
    "python generate_tfrecord.py --csv_input=<path_to_test_labels.csv> --image_dir=<path_to_test_images> --output_path=test.record\n",
    "```\n",
    "\n",
    "*notes: don't forget to adjust each <path> for csv input and image dir*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a986a00-436f-496a-8a26-2f0fe880c8df",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111521c9-652b-42c5-afe5-60c954540b9e",
   "metadata": {},
   "source": [
    "For the modeling part and further, we provide different notebook \"object_detection_modeling.ipynb\". We suggest to run it using GPU environment. Hence, we also provide the collab notebook \n",
    "\n",
    "Few thinngs to be considered before proceeding to modeling step is : \n",
    "- Make sure to have train.record and test.record\n",
    "- Make sure to have labelmap.pbtxt with correct labels mapping"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "batan",
   "language": "python",
   "name": "batan"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
